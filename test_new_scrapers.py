#!/usr/bin/env python3
"""æ¸¬è©¦æ–°çˆ¬èŸ²æ¨¡çµ„"""

import asyncio
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__)))

from app.scrapers.sanjing import SanjingScraper
from app.scrapers.pchome import PChomeScraper
from app.scrapers.momo import MomoScraper
from app.scrapers.gh3c import GH3CScraper

async def test_scraper(scraper_class, test_query="RX 9070"):
    """æ¸¬è©¦å–®å€‹çˆ¬èŸ²"""
    print(f"\n{'='*50}")
    print(f"æ¸¬è©¦ {scraper_class.__name__}")
    print(f"{'='*50}")
    
    try:
        async with scraper_class() as scraper:
            print(f"ðŸ” æœå°‹é—œéµå­—: {test_query}")
            products = await scraper.search_products(test_query)
            
            print(f"ðŸ“Š æ‰¾åˆ° {len(products)} å€‹ç”¢å“:")
            
            if products:
                # é¡¯ç¤ºå‰5å€‹ç”¢å“
                for i, product in enumerate(products[:5], 1):
                    stock_status = "âœ… æœ‰åº«å­˜" if product.in_stock else "âŒ ç„¡åº«å­˜"
                    print(f"{i}. {product.product_name}")
                    print(f"   ðŸ’° åƒ¹æ ¼: NT${product.price:,.0f}")
                    print(f"   ðŸ“¦ åº«å­˜: {stock_status}")
                    print(f"   ðŸ”— é€£çµ: {product.url}")
                    print(f"   ðŸª åº—å®¶: {product.store}")
                    print()
                
                if len(products) > 5:
                    print(f"... é‚„æœ‰ {len(products) - 5} å€‹ç”¢å“")
            else:
                print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç”¢å“")
    
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

async def test_all_scrapers():
    """æ¸¬è©¦æ‰€æœ‰æ–°çˆ¬èŸ²"""
    print("ðŸš€ é–‹å§‹æ¸¬è©¦æ–°å¢žçš„çˆ¬èŸ²æ¨¡çµ„")
    print("æ¸¬è©¦é—œéµå­—: RX 9070")
    
    scrapers_to_test = [
        SanjingScraper,   # ä¸‰äº•3Cè³¼ç‰©ç¶²
        PChomeScraper,    # PChome 24hè³¼ç‰©
        MomoScraper,      # momoè³¼ç‰©ç¶²
        GH3CScraper,      # è‰¯èˆˆé›»å­
    ]
    
    for scraper_class in scrapers_to_test:
        await test_scraper(scraper_class)
        await asyncio.sleep(2)  # é¿å…è«‹æ±‚å¤ªé »ç¹
    
    print(f"\n{'='*50}")
    print("ðŸŽ‰ æ‰€æœ‰çˆ¬èŸ²æ¸¬è©¦å®Œæˆï¼")
    print(f"{'='*50}")

async def test_specific_scraper(scraper_name):
    """æ¸¬è©¦ç‰¹å®šçˆ¬èŸ²"""
    scraper_mapping = {
        "sanjing": SanjingScraper,
        "pchome": PChomeScraper,
        "momo": MomoScraper,
        "gh3c": GH3CScraper,
    }
    
    if scraper_name.lower() in scraper_mapping:
        await test_scraper(scraper_mapping[scraper_name.lower()])
    else:
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¬èŸ²: {scraper_name}")
        print(f"å¯ç”¨çš„çˆ¬èŸ²: {', '.join(scraper_mapping.keys())}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        # æ¸¬è©¦ç‰¹å®šçˆ¬èŸ²
        scraper_name = sys.argv[1]
        asyncio.run(test_specific_scraper(scraper_name))
    else:
        # æ¸¬è©¦æ‰€æœ‰çˆ¬èŸ²
        asyncio.run(test_all_scrapers()) 